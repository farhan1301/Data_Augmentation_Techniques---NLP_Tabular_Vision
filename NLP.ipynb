{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1392b23f",
   "metadata": {},
   "source": [
    "# Install and import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54e9336c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: xgbse 0.2.3 has a non-standard dependency specifier pandas>=1.0.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of xgbse or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install the most recent version of gensim.\n",
    "# Otherwise, you may get the following error when running naw.WordEmbsAug():\n",
    "# 'Word2VecKeyedVectors' object has no attribute 'index_to_key'\n",
    "# see: https://stackoverflow.com/questions/71032760/word2veckeyedvectors-object-has-no-attribute-index-to-key\n",
    "!pip install --upgrade gensim --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db60090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3.2\n"
     ]
    }
   ],
   "source": [
    "# Import gensim.\n",
    "# Note: You will need to retart runtime in order to import the most recent version of gensim \n",
    "import gensim\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa15ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: xgbse 0.2.3 has a non-standard dependency specifier pandas>=1.0.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of xgbse or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install the transformers module in order to use their base models (e.g., BERT)\n",
    "!pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18a2766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transformers\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fc4256c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.37.2\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4a2d459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: xgbse 0.2.3 has a non-standard dependency specifier pandas>=1.0.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of xgbse or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install the tokenizer needed by the back translation model\n",
    "!pip install sacremoses --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0757191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the tokenizer\n",
    "import sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ee2dd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: xgbse 0.2.3 has a non-standard dependency specifier pandas>=1.0.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of xgbse or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install the nlpaug module\n",
    "!pip install nlpaug --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5f2d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the nlpaug module and its methods\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "from nlpaug.util import Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea8b5e",
   "metadata": {},
   "source": [
    "# Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c94600ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?export=download&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
      "From (redirected): https://drive.google.com/uc?export=download&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&confirm=t&uuid=ab70fa65-6126-4305-9bfb-f567e1192e2c\n",
      "To: /Users/farhan/Downloads/Tutorial-3/GoogleNews-vectors-negative300.bin.gz\n",
      "100%|██████████████████████████████████████| 1.65G/1.65G [01:13<00:00, 22.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download models to a temporary path\n",
    "from nlpaug.util.file.download import DownloadUtil\n",
    "DownloadUtil.download_word2vec(dest_dir = '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "294a41d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: ./Models\n",
      "Model already exists in ./Models. Skipping download.\n",
      "Downloading model to ./Models...\n",
      "Downloading model to ./Models...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nlpaug.util.file.download import DownloadUtil\n",
    "\n",
    "def ensure_directory_exists(directory):\n",
    "    \"\"\"Ensure the directory exists. If not, create it.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {directory}\")\n",
    "\n",
    "def download_model_if_not_exists(dest_dir, model_function, model_name=None):\n",
    "    # Determine the model's directory or file path based on the dest_dir and model_name\n",
    "    if model_name:  # If a model_name is provided, use it to adjust the check\n",
    "        model_path = os.path.join(dest_dir, model_name)\n",
    "    else:\n",
    "        model_path = dest_dir  # If no model_name, the check is broader\n",
    "    \n",
    "    # Check if the model directory or specific model file exists\n",
    "    if os.path.exists(model_path) and (not model_name or len(os.listdir(model_path)) > 0):\n",
    "        print(f\"Model already exists in {dest_dir}. Skipping download.\")\n",
    "    else:\n",
    "        print(f\"Downloading model to {dest_dir}...\")\n",
    "        if model_name:\n",
    "            model_function(dest_dir=dest_dir, model_name=model_name)\n",
    "        else:\n",
    "            model_function(dest_dir=dest_dir)\n",
    "\n",
    "# Create 'Models' directory if it doesn't exist\n",
    "models_dir = os.path.join('.', 'Models')\n",
    "ensure_directory_exists(models_dir)\n",
    "\n",
    "# Adjusted example usage\n",
    "\n",
    "# Since download_word2vec does not take a model_name, we call it without model_name\n",
    "download_model_if_not_exists(models_dir, DownloadUtil.download_word2vec)\n",
    "\n",
    "# For fasttext and glove, we continue to pass model_name as before\n",
    "download_model_if_not_exists(models_dir, DownloadUtil.download_fasttext, 'crawl-300d-2M')\n",
    "download_model_if_not_exists(models_dir, DownloadUtil.download_glove, 'glove.6B')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cfffc0",
   "metadata": {},
   "source": [
    "# Example Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee8597a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define some texts\n",
    "text = \"\"\"\n",
    "  Is daily coffee consumption good for our health? \n",
    "  I guess it is reasonable to believe so, but it may also depend on how much you drink.\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cd6814",
   "metadata": {},
   "source": [
    "# Option 1: Substitute or insert word randomly using word embeddings similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c71b707b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "  Is daily coffee consumption good for our health? \n",
      "  I guess it is reasonable to believe so, but it may also depend on how much you drink.\n",
      "  \n",
      "Augmented Text:\n",
      "['Publicity_Stunt Alam_Al_Yawm coffee consumption marvelous KF_OOE our Abortion_foes_capitalize? Personally guess it is unrealistic to believe so, Even it may also depend on spokeswoman_Julie_Zawisza much you drink.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the augmenter with model \"word2vec\" --\n",
    "aug = naw.WordEmbsAug(\n",
    "  # You can choose from \"word2vec\", \"glove\", or \"fasttext\" \n",
    "  model_type = 'word2vec', \n",
    "  model_path = 'GoogleNews-vectors-negative300.bin',\n",
    "  # You may also choose \"insert\"\n",
    "  action = \"substitute\")\n",
    "\n",
    "# Augment the text\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e475af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "  Is daily coffee consumption good for our health? \n",
      "  I guess it is reasonable to believe so, but it may also depend on how much you drink.\n",
      "  \n",
      "Augmented Text:\n",
      "['Is daily coffee.That consumed.The good for our health? lol.I Hmmm. it considers reasonable trying believe so, beause it chould also depend on how much.Why you drink.']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the augmenter with model \"fasttext\"\n",
    "aug = naw.WordEmbsAug(\n",
    "  # You can choose from \"word2vec\", \"glove\", or \"fasttext\" \n",
    "  model_type = 'fasttext', \n",
    "  # Note: check your \"content\" path to find out specific model names\n",
    "  model_path = 'Models/crawl-300d-2M.vec',\n",
    "  # You may also choose \"insert\"\n",
    "  action = \"substitute\")\n",
    "\n",
    "# Augment the text\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ec5a8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "  Is daily coffee consumption good for our health? \n",
      "  I guess it is reasonable to believe so, but it may also depend on how much you drink.\n",
      "  \n",
      "Augmented Text:\n",
      "['Is mirror coffee drinking guys the our health? I my anything making reasonable to believe so, but it may also livelihoods on did much you drink.']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the augmenter with model \"glove\"\n",
    "aug = naw.WordEmbsAug(\n",
    "  # You can choose from \"word2vec\", \"glove\", or \"fasttext\" \n",
    "  model_type = 'glove', \n",
    "  # Note: check your \"content\" path to find out specific model names\n",
    "  model_path = 'Models/glove.6B.300d.txt',\n",
    "  # You may also choose \"insert\"\n",
    "  action = \"substitute\")\n",
    "\n",
    "# Augment the text\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded5ebc8",
   "metadata": {},
   "source": [
    "# Option 2: Substitute or insert word by contextual word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b924abd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "  Is daily coffee consumption good for our health? \n",
      "  I guess it is reasonable to believe so, but it may also depend on how much you drink.\n",
      "  \n",
      "Augmented Text:\n",
      "['is daily activity just good in our needs? people felt it was reasonable to say otherwise, but it may also depend on how much you drink.']\n"
     ]
    }
   ],
   "source": [
    "## Substitute word by contextual word embeddings (BERT, DistilBERT, RoBERTA or XLNet)\n",
    "aug = naw.ContextualWordEmbsAug(\n",
    "  # Other models include 'distilbert-base-uncased', 'roberta-base', etc.\n",
    "  model_path = 'bert-base-uncased', \n",
    "  # You can also choose \"insert\"\n",
    "  action = \"substitute\")\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535e8fde",
   "metadata": {},
   "source": [
    "# Option 3: Substitute or insert word by synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97c69656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "  Is daily coffee consumption good for our health? \n",
      "  I guess it is reasonable to believe so, but it may also depend on how much you drink.\n",
      "  \n",
      "Augmented Text:\n",
      "['Is daily coffee wasting disease unspoilt for our health? I guess information technology be sensible to believe so, but it may also depend on how much you drink.']\n"
     ]
    }
   ],
   "source": [
    "## Substitute word by WordNet's synonym\n",
    "aug = naw.SynonymAug(aug_src = 'wordnet')\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23875a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "  Is daily coffee consumption good for our health? \n",
      "  I guess it is reasonable to believe so, but it may also depend on how much you drink.\n",
      "  \n",
      "Augmented Text:\n",
      "['Is daily coffee consumption good for our health? I guess it is reasonable to believe so, simply information technology may also depend on how much you drink.']\n"
     ]
    }
   ],
   "source": [
    "## Substitute word by WordNet's synonym.\n",
    "# You can optionally set the max number of words to replace with synonym.\n",
    "aug = naw.SynonymAug(aug_src = 'wordnet', aug_max = 3)\n",
    "augmented_text = aug.augment(text, )\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e70efd",
   "metadata": {},
   "source": [
    "# Option 4: Substitute or insert word using back translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dea0e1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of FSMTForConditionalGeneration were not initialized from the model checkpoint at facebook/wmt19-en-de and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of FSMTForConditionalGeneration were not initialized from the model checkpoint at facebook/wmt19-de-en and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Is daily coffee consumption good for our health? I think it is reasonable to believe so, but it can also depend on how much you drink.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use back translation augmenter\n",
    "back_translation_aug = naw.BackTranslationAug(\n",
    "    from_model_name = 'facebook/wmt19-en-de', \n",
    "    to_model_name = 'facebook/wmt19-de-en'\n",
    ")\n",
    "back_translation_aug.augment(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
